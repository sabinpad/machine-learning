{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544a2dc7",
   "metadata": {},
   "source": [
    "# Company Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4bab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, silhouette_score\n",
    "\n",
    "from utils.preprocessing import (\n",
    "    IndustryGrouper,\n",
    "    TextGrouper,\n",
    "    IndustrySectorGrouper,\n",
    "    DescriptionSectorGrouper,\n",
    "    ExtendedSectorGrouper,\n",
    "    TaxonomyVectorizer\n",
    ")\n",
    "\n",
    "# NOTE dataset\n",
    "\n",
    "companies_df = pd.read_csv(\"../data/companies.csv\")\n",
    "\n",
    "# initial preprocessing\n",
    "\n",
    "companies_df[\"business_tags\"] = companies_df[\"business_tags\"].apply(ast.literal_eval)\n",
    "companies_df[\"description\"] = companies_df[\"description\"].fillna(\"\")\n",
    "companies_df = companies_df[companies_df[\"category\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# NOTE configuration variables\n",
    "\n",
    "CLUSTERS_NUMBER = 220\n",
    "TRAIN_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ad8ab",
   "metadata": {},
   "source": [
    "## Target Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbf9ca",
   "metadata": {},
   "source": [
    "### Feature Constuction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01568b8",
   "metadata": {},
   "source": [
    "To construct the target feature it is necessary to figure out the features that\n",
    "the clustering model uses. This is important because the labels are infered\n",
    "based on these features.\n",
    "\n",
    "Clustering, groups data that is close toghether in the feature space. Because\n",
    "of this it is mandatory to choose features that separate data well. In our case\n",
    "because of the vectorization, we need a small to medium number or keywords that\n",
    "place a description or tags about a company near each other in the reference\n",
    "space.\n",
    "\n",
    "At the same it is not necessary to have a large number of keywords because\n",
    "the clustering is more expensive and won't separate the data well enough. There\n",
    "are better embeddings than solve the problem of meaning and ordering, discussed\n",
    "in the README, than TF-IDF does.\n",
    "\n",
    "Further more, it is not known if all taxonomy classes are present in the\n",
    "dataset. For this step it is assumed that every taxonomy class appears at least\n",
    "once.\n",
    "\n",
    "The considered approaches are:\n",
    "* clustering based on vectorization of *sector* and *business_tags*\n",
    "* clustering based on vectorization of *sector*, *category* and *niche* features\n",
    "* clustering based on one-hot encoding of the *sector* feature and vectorization of *category* and *niche* features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "construct_pipelines = [\n",
    "    (\"all\", Pipeline([\n",
    "        (\"grouper\", TextGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "    ])),\n",
    "    (\"industry\", Pipeline([\n",
    "        (\"grouper\", IndustryGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "    ])),\n",
    "    (\"categorical\", Pipeline([\n",
    "        (\"grouper\", IndustrySectorGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"encoder\", OneHotEncoder(), [\"sector\"]),\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# training - takes about 30s\n",
    "\n",
    "names = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for name, constr_pipeline in construct_pipelines:\n",
    "    constr_pipeline.fit_predict(companies_df)\n",
    "\n",
    "    names.append(name)\n",
    "    score = silhouette_score(constr_pipeline.transform(companies_df), constr_pipeline.named_steps[\"clusterer\"].labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# plotting\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(names, silhouette_scores)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485daf6",
   "metadata": {},
   "source": [
    "By looking at the bar plot, the industry features have the lowest silhouette\n",
    "score overall. This means that this combination of features, when vectorized,\n",
    "meaning being put into a reference space, describe the clusters best.\n",
    "\n",
    "The actual lowest value corresponds to the first preprocessing but it is\n",
    "explained later why this is not a good indicator.\n",
    "\n",
    "> Sometimes the scores can vary from iteration to iteration because of the\n",
    "> nature of the **KMeans** model. But over many iterations the score of the\n",
    "> industry features is always the lowest.\n",
    "\n",
    "The reason why these features fit the data well is because the texts that\n",
    "resulted from the grouping of the individual strings in each feature don't\n",
    "contain many different words so the clusterer si able to separate them well in\n",
    "the reference space and group them correctly.\n",
    "\n",
    "Another reason why the industry features are a good choice is because they\n",
    "contain just the right number of words to cluster the data well while not being\n",
    "computationally expensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE construction pipeline based on bar plot\n",
    "\n",
    "construct_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d734b",
   "metadata": {},
   "source": [
    "### Number of taxonomy classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb754cb",
   "metadata": {},
   "source": [
    "There is the posibility that not all taxonomy classes apear in the original\n",
    "dataset. The distribution (not really a distribution because classes are\n",
    "categorical) of the taxonomy classes might not be uniform. This tells us that\n",
    "maybe not all taxonomy classes are used and a clustering approach for all of\n",
    "them will not work as intended.\n",
    "\n",
    "To test this hypothesis a metric is used to determine the *K* hyperparameter that\n",
    "clusters the dataset best.\n",
    "\n",
    "An elbow plot is the most common plot to evaluate when it comes to choosing *K*.\n",
    "Based on this plot, the *k* parameter that works best is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training scores\n",
    "\n",
    "with open(\"../data/cluster_models_inertias.pkl\", \"rb\") as f:\n",
    "    points = pickle.load(f)\n",
    "\n",
    "clusters_numbers, inertias = points\n",
    "\n",
    "# plotting\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(clusters_numbers, inertias)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782682ae",
   "metadata": {},
   "source": [
    "Because the elbow plot doesn't show a clear \"elbow\" line it is hard to interpret\n",
    "the data and conclude if there is a value for K that clusters the data best.\n",
    "\n",
    "It is also possible that the company for which we do the analysis just needs to\n",
    "classify the dataset using all taxonomy classes. In that case the elbow plot\n",
    "is not important anymore since all classes are required.\n",
    "\n",
    "Because of the reasons mentioned above from now it is assumed that all taxonomy\n",
    "classes are required and appear at least once in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d073f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE number of clusters based on elbow plot\n",
    "\n",
    "construct_pipeline.named_steps[\"clusterer\"] = KMeans(CLUSTERS_NUMBER)\n",
    "\n",
    "companies_df[\"taxonomy\"] = construct_pipeline.fit_predict(companies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59565172",
   "metadata": {},
   "source": [
    "## Training & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590557c",
   "metadata": {},
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a75c7",
   "metadata": {},
   "source": [
    "A good data representation (embedding) can have significantly improve the\n",
    "accuracy of the model. Because of this, models require a good preprocessing.\n",
    "\n",
    "Training is different from feature cronstruction because if the latter requires\n",
    "diverse data to compute complex relationships between features while in the\n",
    "former requires clear separation of data for grouping.\n",
    "\n",
    "Because of this the preprocessing approaches consider almost all features for\n",
    "training so that the model learns to use different similar words for the same\n",
    "taxonomy label.\n",
    "\n",
    "The considered approaches are:\n",
    "* classification based on vectorization of all features\n",
    "* classification based on *sector*, *category* and *niche* features (should\n",
    "capture the most important words and be less expensive)\n",
    "* classification based on one-hot encoding of the *sector* feature and\n",
    "vectorization of all other features\n",
    "* classification based on one-hot encoding of the *sector* feature and\n",
    "vectorization of *business_tags* and *description* and (should have the same effect but be less expensive)\n",
    "\n",
    "In order to test multiple preprocessing approaches, logistic regression is\n",
    "chosen as the initial model. Since it uses the softmax function to compute\n",
    "probabilities, it represents a good starting point for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c358c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6aef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pipelines\n",
    "\n",
    "classification_pipelines = [\n",
    "    (\"all\", Pipeline([\n",
    "        (\"grouper\", TextGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"classifier\", LogisticRegression())\n",
    "    ])),\n",
    "    (\"industry\", Pipeline([\n",
    "        (\"grouper\", IndustryGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"classifier\", LogisticRegression())\n",
    "    ])),\n",
    "    (\"other_and_categorical\", Pipeline([\n",
    "        (\"grouper\", ExtendedSectorGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"encoder\", OneHotEncoder(), [\"sector\"]),\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"classifier\", LogisticRegression())\n",
    "    ])),\n",
    "    (\"tags_and_categorical\", Pipeline([\n",
    "        (\"grouper\", DescriptionSectorGrouper()),\n",
    "        (\"col_transformer\", ColumnTransformer([\n",
    "            (\"encoder\", OneHotEncoder(), [\"sector\"]),\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "        ])),\n",
    "        (\"classifier\", LogisticRegression())\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# training - takes about 3-5min\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(companies_df.drop([\"taxonomy\"], axis=1), companies_df[\"taxonomy\"], train_size=TRAIN_RATIO)\n",
    "\n",
    "names = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for name, clf_pipeline in classification_pipelines:\n",
    "    clf_pipeline.fit(X_train, t_train)\n",
    "\n",
    "    y_test = clf_pipeline.predict(X_test)\n",
    "\n",
    "    names.append(name)\n",
    "    accuracy_scores.append(accuracy_score(t_test, y_test))\n",
    "\n",
    "# plotting\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(names, accuracy_scores)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde28688",
   "metadata": {},
   "source": [
    "### Training Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa841aa",
   "metadata": {},
   "source": [
    "First will take a look at why the first clustering method, with the best\n",
    "silhouette score, groups companies in the wrong way.\n",
    "\n",
    "Because the vocabulary, after vectorization, contains a lot of distinct words,\n",
    "the data also has high dimensionality which leads to harder grouping and a\n",
    "higher computational cost and an overall higher. Because the number of features\n",
    "is significantly larger than the number of lines, the model isn't able to learn\n",
    "the patterns necesarry to make good predictions.\n",
    "\n",
    "Because the vocabulary, after vectorization, contains a lot of distinct words,\n",
    "the data also has high dimensionality. This leads to a higher computational cost\n",
    "and an overall harder grouping. Because the number of features is significantly\n",
    "larger than the number of entries int the dataset, the model isn't able to\n",
    "properly learn the patterns necesarry to make good predictions.\n",
    "\n",
    "This can be seen in the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9865a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "construct_pipeline = Pipeline([\n",
    "    (\"grouper\", TextGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "])\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# training - takes about 40s\n",
    "\n",
    "companies_df[\"taxonomy\"] = construct_pipeline.fit_predict(companies_df)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(companies_df.drop([\"taxonomy\"], axis=1), companies_df[\"taxonomy\"], train_size=TRAIN_RATIO)\n",
    "\n",
    "classification_pipeline.fit(X_train, t_train)\n",
    "\n",
    "y_test = classification_pipeline.predict(X_test)\n",
    "\n",
    "# scores\n",
    "\n",
    "print(classification_report(t_test, y_test, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82aa21",
   "metadata": {},
   "source": [
    "Another idea is represented by the posibility of using the same embedding\n",
    "but pre-trained using specific words, meaning a predefined reference space. This\n",
    "approach has the advantage of capturing only the words that best describe each\n",
    "cluster while also reducing the feature space, which in turn reduces the\n",
    "computational cost. To make sure the clustering is effective, all features are\n",
    "vectorized to capture as much words as possible.\n",
    "\n",
    "The problem with this approach arises in the training phase. Because the feature\n",
    "space has small to medium dimensionality, the classification models can't learn\n",
    "complex patterns that help them classify the companies better. This leads to\n",
    "bad accuracy and doesn't scale well to larger datasets and datasets with higher\n",
    "dimensionality (the vectorization basically filters out all words except\n",
    "the ones that it has been trained with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f373fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "construct_pipeline = Pipeline([\n",
    "    (\"grouper\", TextGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TaxonomyVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "])\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# training - takes about 15s\n",
    "\n",
    "companies_df[\"taxonomy\"] = construct_pipeline.fit_predict(companies_df)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(companies_df.drop([\"taxonomy\"], axis=1), companies_df[\"taxonomy\"], train_size=TRAIN_RATIO)\n",
    "\n",
    "classification_pipeline.fit(X_train, t_train)\n",
    "\n",
    "y_test = classification_pipeline.predict(X_test)\n",
    "\n",
    "# scores\n",
    "\n",
    "print(classification_report(t_test, y_test, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88a8c3",
   "metadata": {},
   "source": [
    "\n",
    "Changing the training pipeline to also use the pre-trained vectorizer while\n",
    "learning using all features seems to be better.\n",
    "\n",
    "This proves to have a better accuracy but the accuracy is misleading because in\n",
    "this case the model may overfit. Because the feature space is small, and the\n",
    "words are repetitive, the model tends to memorize the dataset instead of\n",
    "learning relationships between features.  to  doesn't scale well to larger\n",
    "dataset because it takes and can't learn complex patterns\n",
    "\n",
    "This also doesn't scale well to larger datasets and dataset with more features\n",
    "because it can't learn complex patterns due to the fixed vectorization used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7659a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "construct_pipeline = Pipeline([\n",
    "    (\"grouper\", TextGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TaxonomyVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "])\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    (\"grouper\", TextGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TaxonomyVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# training - takes about 15s\n",
    "\n",
    "companies_df[\"taxonomy\"] = construct_pipeline.fit_predict(companies_df)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(companies_df.drop([\"taxonomy\"], axis=1), companies_df[\"taxonomy\"], train_size=TRAIN_RATIO)\n",
    "\n",
    "classification_pipeline.fit(X_train, t_train)\n",
    "\n",
    "y_test = classification_pipeline.predict(X_test)\n",
    "\n",
    "# scores\n",
    "\n",
    "print(classification_report(t_test, y_test, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fec0d",
   "metadata": {},
   "source": [
    "Another aspect is trying to see if all taxonomy classes are used in the\n",
    "original dataset.\n",
    "\n",
    "In theory, if the entries are placed well enough in the feature space, the exact\n",
    "number of cluster, and therefore taxonomy classes, can be found using DBSCAN.\n",
    "The only problem to this approach is the *epsilon* hyperpatameter which may, or\n",
    "may not be hard to find.\n",
    "\n",
    "In this example the model has a pretty high accuracy but the value is a wrong\n",
    "indicator because it classifies the majority of the companies using the same\n",
    "taxonomy class which is not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "construct_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clusterer\", DBSCAN(1))\n",
    "])\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "        (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# training - takes about 15s\n",
    "\n",
    "companies_df[\"taxonomy\"] = construct_pipeline.fit_predict(companies_df)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(companies_df.drop([\"taxonomy\"], axis=1), companies_df[\"taxonomy\"], train_size=TRAIN_RATIO)\n",
    "\n",
    "classification_pipeline.fit(X_train, t_train)\n",
    "\n",
    "y_test = classification_pipeline.predict(X_test)\n",
    "\n",
    "# scores\n",
    "\n",
    "print(classification_report(t_test, y_test, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5ba1a",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eeeb0f",
   "metadata": {},
   "source": [
    "Based on the observations made, the TF-IDF vectorization of the concatenation\n",
    "of the *sector*, *category* and *niche* features provides a solid \"reference\"\n",
    "space for grouping companies and training models to efficiently classify these\n",
    "companies while also maintaining a low computational cost which is a necessity\n",
    "when scaling models for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE final construction and classification pipelines\n",
    "\n",
    "construct_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clusterer\", KMeans(CLUSTERS_NUMBER))\n",
    "])\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    (\"grouper\", IndustryGrouper()),\n",
    "    (\"col_transformer\", ColumnTransformer([\n",
    "            (\"vectorizer\", TfidfVectorizer(), \"words\")\n",
    "    ])),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# training - takes about 15s\n",
    "\n",
    "companies_df[\"taxonomy\"] = construct_pipeline.fit_predict(companies_df)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(companies_df.drop([\"taxonomy\"], axis=1), companies_df[\"taxonomy\"], train_size=TRAIN_RATIO)\n",
    "\n",
    "classification_pipeline.fit(X_train, t_train)\n",
    "\n",
    "y_test = classification_pipeline.predict(X_test)\n",
    "\n",
    "# scores\n",
    "\n",
    "print(classification_report(t_test, y_test, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
