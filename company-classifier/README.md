Copyright Sabin Padurariu (c) 2025

# Company Classifier

## Overview & Remarks

The task is classifying companies into an existing insurance taxonomy without
having the dataset labeled beforehand.

I kept things simple and used general machine learning models because a good
solution doesn't need to be complex, it needs to be effective and straight to
the point.

I tried to explain the reasons behind every decision I made and also prove it
by plotting important scores and results.

## Exploratory Data Analysis

The dataset contains mainly words. The words are grouped in sentences, lists
of tags and descriptions but they also appear just as single words.

Machine learning models can't work with words, or any non-numeric data types
for that matter, so the data needs to be converted into some kind of numeric
representation (embedding).

For this challenge I chose to work with the TF-IDF representation because
it is a fairly simple representation that also adds context/information to
words when converted.

Even though every column could be interpreted as one or more words, the
*sector* feature has only single words as values. Because of this, it can
be also treated as a categorical feature and a plot can be made to inspect
the frequency of each class.

![sector plot](./docs/images/sector.png)

The plot clearly shows that there are more companies in the manufacturing
sector than in other sectors. This could be an insight for understanding the
dataset but could also pose a problem that leads to poor generalization.

Another important analysis is over the disctinct number of words that the
TF-IDF algorithm produces. The number of distinct words generated by a TF-IDF
vectorizer is important because it directly impacts the quality and
effectiveness of the text representation.

![vectorization plot](./docs/images/vectorization.png)

A large number of distinct words usually means the vectorizer captures a rich
and detailed vocabulary but the processing cost of the vector is higher. On the
other hand, a small number of distinct words may simplify the representation
and reduce noise, but it risks losing important information. Therefore, there
needs to be a balance between the vocabulary size and the vocabulary content. 

The dataset also contains some entries with missing values. 

Because these values are actual words and descriptions, they cannot be imputated
using techniques such as mean imputation or median imputations. Instead they
need to be filled in manually

Also, because there is a relatively small number of entries with missing values
compared to the size of the dataset, some entries are removed. This isn't going
to affect the performance of the models.

## Key Ideas and Approaches

The main idea comes from the fact that companies need to be classified using
words. That means that the model needs an embedding that can capture the
information provided by the words. This is the reason behind choosing TF-IDF. 

The key idea is that words are finite and have meaning. So even words can have
a pretty fixed numeric represenation, although a very loose one, that depends
on the context and ordering. Because of this, a "reference" space can be
created, where sentences/documents that contain roughly the same words can be
grouped closed to each other or be separated based on their content.

The main difficulty comes from knowing how to choose the reference space and
what features that contain words to use for training

Also, because one column contains only single words, those words can be labeled
and interpreted in a different way which can improve the learning process.

We can think of words, just for the sake of the example, as labels of a certain
language.

```C
enum word {
    go,
    run,
    play,
    playing,
    red,
    concrete,
    discombobulate
}
```

So if a description contains the word "concrete", then probably the document
is about construction.

Right of the bat one problem arises. Some words also have conjugate forms. And
because of this, vectorizations like TF-IDF won't be able to match "play" and
"playing" because it treats them as two separate labels. The idea behind TF-IDF,
that is also used in this solution, is that we need enough words from a domain,
given a reference space, so that the algorithm will place sentences near each
other and cluster them.

There is also the word ordering part. But we don't care about this aspect in our
case. It is enough to consider the meaning of words and not the ordering as well.

Another issue stems from the frequencies of the classes of the *sector* feature.

There are a lot of companies that do their work in the manufacturing sector.
Because of this, the frequencies between the classes are not equal and that
makes training and predicting harder for the other classes because of lack of
data. There is not enough diversity to train the model properly and the model
might overfit.

The plot could also indicate that maybe there are more companies
that manufacture goods, sell products or sell services than there are doing
other types of work. This could prove to be an insight, not a problem. In our
case it is not really important whether a company is selling en-gros or directly
to customers. They just need to be classified well using the taxonomy labels
provided. It could be tried, and it should be tried, to train the model for
more diverse companies but at the end of the day more data is needed. 
